{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Presentation du projet d'indexion de texte</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "import string\n",
    "import operator\n",
    "import shutil,os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Represent the constraint of memory\n",
    "NB_DOCUMENT = 1000\n",
    "DATA_PATH = \"../data/latimes/\"\n",
    "WRITING_PATH_POSTING_LIST = \"../data/save/\"\n",
    "NAME_POSTING_LIST = \"postingList_\"\n",
    "SEPARATOR = \" \"\n",
    "NAME_DOC_LIST = \"docList\"\n",
    "\n",
    "# link the tags with the importance in the text.\n",
    "TAGS_IMPORTANCE = {  'headline': 3,\n",
    "                     'text': 1,\n",
    "                     'section':1,\n",
    "                     'graphic':2\n",
    "                  }\n",
    "STOP_WORDS = stopwords.words('english') + list(string.punctuation)\n",
    "STEMMER = PorterStemmer()\n",
    "TAG_NUMBER = \"NUMBER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "# clean repository of the giver path: \"folder\"\n",
    "###\n",
    "def cleanRepository(folder):\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            #elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Tokenize a sentense.\n",
    "##\n",
    "def tokenizeWord(paragraphContent):  \n",
    "    # We tokenize and remove the stop word\n",
    "    words = [word for word in word_tokenize(paragraphContent.lower()) if word not in STOP_WORDS]\n",
    "    \n",
    "    # nlkt does not decompose the hyphen.\n",
    "    splitHiphen = []\n",
    "    for word in words:\n",
    "        if '-' in word:\n",
    "            for decomposedWord in word.split('-'):\n",
    "                splitHiphen.append(decomposedWord)\n",
    "        else:\n",
    "            splitHiphen.append(word)  \n",
    "            \n",
    "    return splitHiphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Format the text in the right form.\n",
    "# Tokenize and stem the text\n",
    "# Update the voc list passed in parameter.\n",
    "##\n",
    "def handleFormatText(paragraphContent, vocList, docLenght, docId):  \n",
    "    # We tokenize and remove the stop word\n",
    "    words = tokenizeWord(paragraphContent) \n",
    "    \n",
    "    stemWords = []\n",
    "    # We loop on each word.\n",
    "    for word in words:\n",
    "        stemWord = STEMMER.stem(word)\n",
    "        \n",
    "        # Selection on a part of string.\n",
    "        stemWord = re.sub(\"[*\\'\\.+:,\\`:/]\", '', stemWord)\n",
    "        if stemWord.isdigit() or len(stemWord) < 2:\n",
    "            continue\n",
    "            \n",
    "        stemWords.append(stemWord)\n",
    "        # Update the listVoc\n",
    "        if stemWord in vocList:\n",
    "            vocList[stemWord] = vocList[stemWord] + 1\n",
    "        else:\n",
    "            vocList[stemWord] = 1\n",
    "        \n",
    "        docLenght[docId] += 1\n",
    "    return stemWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "# The function add the entry in the correct posting list\n",
    "##\n",
    "def buildPostingList(stemWords, currentDict, idDoc):\n",
    "    # We update the stemWords.\n",
    "    for word in stemWords:\n",
    "        # The word have already been seen, we update thedict\n",
    "        if word in currentDict :\n",
    "            # We update the dict reprensenting the posting list.\n",
    "            if idDoc in currentDict[word]:\n",
    "                currentDict[word][idDoc] = currentDict[word][idDoc] + 1\n",
    "\n",
    "            else:\n",
    "                currentDict[word][idDoc] = 1\n",
    "\n",
    "        # We don't have word for now\n",
    "        else:\n",
    "            currentDict[word] = {idDoc : 1};\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Write file.\n",
    "##\n",
    "def writingInFile(currentDict, index, path, name, separator):  \n",
    "    # sort word for the posting list.\n",
    "    sorted_word = sorted(currentDict.keys())\n",
    "    \n",
    "    # write the posting list.\n",
    "    with open(path+name+str(index),\"a+\") as f:\n",
    "        for word in sorted_word:\n",
    "            portingEntry = word + separator\n",
    "            for docID, value in currentDict[word].items():\n",
    "                portingEntry = portingEntry + str(docID) + separator + str(value) + separator\n",
    "            f.write(portingEntry + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Write voc file\n",
    "##\n",
    "def writingDictInFile(currentDict, path, name, separator): \n",
    "    # write the posting list.\n",
    "    with open(path+name ,\"a+\") as f:\n",
    "        for docID, value in currentDict.items():\n",
    "            portingEntry = \"\"\n",
    "            portingEntry = portingEntry + str(docID) + separator + str(value)\n",
    "            f.write(portingEntry + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "# The function build the index file composed by the voc and the associated posting list.\n",
    "##\n",
    "def buildIndexFile(vocList, docLenght) :\n",
    "    print(\"Building index File\")\n",
    "    \n",
    "    # We get the list of file containing the articles.\n",
    "    articles = [DATA_PATH + file for file in listdir(DATA_PATH) if (isfile(join(DATA_PATH, file)) and \".txt\" not in file and \".DS_Store\" not in file )]\n",
    "    progress_bar = FloatProgress(min=0, max=len(articles))\n",
    "    display(progress_bar)\n",
    "    \n",
    "    # List containing the term and the number of time it appear.\n",
    "    currentPostingList = {}\n",
    "    counter = 0\n",
    "    docIDCounter = 0 \n",
    "    \n",
    "    \n",
    "    #We loop on each document composing the corpus.\n",
    "    for article in articles:\n",
    "        with open(article) as curArticle:\n",
    "            file = curArticle.read()\n",
    "            fileXML = bs(file,\"lxml\")\n",
    "            \n",
    "            # We loop on each doc tag\n",
    "            for document in fileXML.findAll('doc'):\n",
    "                docIDCounter = docIDCounter + 1\n",
    "                docID = document.find(\"docid\").string\n",
    "                docLenght[docID] = 0\n",
    "                \n",
    "                # get the text containing in the current article\n",
    "                curParagraph = document.find_all('p')\n",
    "                for paragraph in curParagraph:\n",
    "                   \n",
    "                    # We balance with the importance of the parent tag\n",
    "                    if paragraph.parent.name in TAGS_IMPORTANCE:\n",
    "                        for index in range(TAGS_IMPORTANCE[paragraph.parent.name]):\n",
    "                            stemWords = handleFormatText(paragraph.string,vocList, docLenght, docID)\n",
    "                            buildPostingList(stemWords, currentPostingList, int(docID))\n",
    "                             \n",
    "                if docIDCounter % NB_DOCUMENT == 0 :\n",
    "                    counter = counter + 1\n",
    "                    writingInFile(currentPostingList, counter, WRITING_PATH_POSTING_LIST, NAME_POSTING_LIST, SEPARATOR)\n",
    "                    # clear the ram memory.\n",
    "                    currentPostingList.clear()\n",
    "             \n",
    "        curArticle.closed\n",
    "        progress_bar.value += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creation du index File</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleanRepository(WRITING_PATH_POSTING_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocList = {}\n",
    "docLenght = {}\n",
    "buildIndexFile(vocList, docLenght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writingDictInFile(docLenght, WRITING_PATH_POSTING_LIST, NAME_DOC_LIST, \" \")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
