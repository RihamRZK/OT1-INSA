{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Presentation of the project of the indexation of files </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "import string\n",
    "import operator\n",
    "import shutil,os\n",
    "import re\n",
    "import sys\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport util_index\n",
    "%aimport util_posting\n",
    "%aimport fagin\n",
    "%aimport graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WRITING_PATH_POSTING_LIST = \"../data/saveFile/\"\n",
    "WRITING_PATH_UTIL = \"../data/util/\"\n",
    "NAME_DOC_LIST = \"docList\"\n",
    "NB_DOCUMENT = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Creation of the index File</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to clean the repository of all old version of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_index.cleanRepository(WRITING_PATH_POSTING_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocList = {}\n",
    "docLenght = {}\n",
    "util_index.buildIndexFile(vocList, docLenght, WRITING_PATH_POSTING_LIST, NB_DOCUMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writing in file, the format is : \"docId\" \"nombre Of word contained in the doc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "util_index.writingDictInFile(docLenght, WRITING_PATH_UTIL, NAME_DOC_LIST, \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Creation Posting List</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_posting.createPostingList()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Graph Generation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbCuts = [10, 50, 100, 500, 1000, 5000, 10000, 50000]\n",
    "result = graph.computeDictRelationWithSizeTime(nbCuts,WRITING_PATH_POSTING_LIST, WRITING_PATH_UTIL, NAME_DOC_LIST)\n",
    "graph.displayResult( result,\n",
    "                    'trade-off size document / time needed to build posting list', \n",
    "                    'log(mean size document in bytes)',\n",
    "                    'time in second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PostingList(object):\n",
    "    # Args :\n",
    "    # qt is a string containing the query term this PL is made for\n",
    "    # ordered_list is a list of (score, doc_id) ordered in decreasing score\n",
    "    # access_dict (optional, can be computed from ordered_list) is a dict associating a doc to its score in this PL\n",
    "    def __init__(self, qt, ordered_list, access_dict=None):\n",
    "        self.qt=qt\n",
    "        self.ordered_list = ordered_list\n",
    "        if access_dict is not None:\n",
    "            self.access_dict = access_dict\n",
    "        else:\n",
    "            self.access_dict = {}\n",
    "            for score,doc in ordered_list:\n",
    "                assert doc not in self.access_dict\n",
    "                self.access_dict[doc] = score\n",
    "\n",
    "        self.docs_visited = set()\n",
    "        self.ordered_idx = 0\n",
    "\n",
    "    # Returns : A (score, doc_index) tuple corresponding to the first non-visited entry in the ordered traversal\n",
    "    def seek_next(self):\n",
    "        while self.ordered_list[self.ordered_idx][1] in self.docs_visited:\n",
    "            self.ordered_idx += 1\n",
    "        return self.ordered_list[self.ordered_idx]\n",
    "\n",
    "    # Returns : The score of the item preceding the next ordered accessed item\n",
    "    def next_item_predecessor_score(self):\n",
    "        tmp_idx = self.ordered_idx\n",
    "        while self.ordered_list[tmp_idx][1] in self.docs_visited:\n",
    "            self.ordered_idx += 1\n",
    "        return self.ordered_list[tmp_idx-1][0]\n",
    "\n",
    "    # Args :\n",
    "    # doc_id is an integer containing the id of the document we want to mark as visited in the sorted access\n",
    "    def mark_visited(self, doc_id):\n",
    "        assert doc_id not in self.docs_visited\n",
    "        self.docs_visited.add(doc_id)\n",
    "\n",
    "    # Args :\n",
    "    # doc_id is an integer containing the document id to lookup in the random access\n",
    "    #\n",
    "    # Returns : The score of the queried document in the PL\n",
    "    def random_lookup(self, doc_id):\n",
    "        return self.access_dict[doc_id]\n",
    "\n",
    "#TODO(mathishammel): Use a real priority queue. Lists are disgusting\n",
    "class TopEntries(object):\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.top = []\n",
    "\n",
    "    def insert(self, priority, element):\n",
    "        self.top += [(priority, element)]\n",
    "        self.top = sorted(self.top, reverse=True)[:self.k]\n",
    "\n",
    "    def pop_lowest(self):\n",
    "        res = self.top[-1]\n",
    "        del self.top[-1]\n",
    "        return res\n",
    "\n",
    "    def get_min_score(self):\n",
    "        if len(self.top) == 0:\n",
    "            return -1.0\n",
    "        return self.top[-1][0]\n",
    "\n",
    "def calc_avg(lst):\n",
    "    return float(sum(lst))/len(lst)\n",
    "\n",
    "# Args :\n",
    "# posting_lists is a list of PostingList objects corresponding to the PLs for all query terms.\n",
    "# k is an integer containing the length of the desired top-k ranking\n",
    "#\n",
    "# Returns : A list containing the (total_score, doc_id) for the top-k elements\n",
    "def top_k_thresh(posting_lists, k, epsilon=0.0):\n",
    "    #Check if we're not trying to get an impossible top-k\n",
    "    for posting_list in posting_lists:\n",
    "        assert k < len(posting_list.ordered_list)\n",
    "\n",
    "    top_k = TopEntries(k)\n",
    "    top_non_visited = []\n",
    "    for posting_list in posting_lists:\n",
    "        top_non_visited.append(posting_list.seek_next())\n",
    "\n",
    "    eprint('Initialized top inorder array :', top_non_visited)\n",
    "    threshold = 1e999 # Close enough to infinity, hopefully...\n",
    "    \n",
    "    while top_k.get_min_score() < threshold / (1.0 + epsilon):\n",
    "        eprint('Starting new round')\n",
    "        selected_idx = -1\n",
    "        best_indiv_in_order = -1.0 # Assuming all PL scores are positive\n",
    "        for idx, score in enumerate(top_non_visited):\n",
    "            if best_indiv_in_order < score:\n",
    "                selected_idx = idx\n",
    "                best_indiv_in_order = score\n",
    "        selected_element = top_non_visited[selected_idx]\n",
    "        selected_score, selected_doc_id = selected_element\n",
    "        eprint('  Selected PL index is', selected_idx)\n",
    "        eprint('  Selected element is', selected_element)\n",
    "        posting_lists[selected_idx].mark_visited(top_non_visited[selected_idx][1])\n",
    "        top_non_visited[selected_idx] = posting_lists[selected_idx].seek_next()\n",
    "\n",
    "        scores = []\n",
    "        for idx in range(len(posting_lists)):\n",
    "            if idx == selected_idx:\n",
    "                scores.append(selected_score)\n",
    "                continue\n",
    "            scores.append(posting_lists[idx].random_lookup(selected_doc_id))\n",
    "            posting_lists[idx].mark_visited(selected_doc_id)\n",
    "            if top_non_visited[idx][1] == selected_doc_id:\n",
    "                top_non_visited[idx] = posting_lists[idx].seek_next()\n",
    "        \n",
    "        eprint('  Individual scores for document',selected_doc_id,'are', scores)\n",
    "        tot_score = calc_avg(scores)\n",
    "        eprint('  Average score is', tot_score)\n",
    "        top_k.insert(tot_score, selected_doc_id)\n",
    "        eprint('  Current top K is', top_k.top)\n",
    "\n",
    "        all_lists_ready = True\n",
    "        next_prev_scores = []\n",
    "        for idx,posting_list in enumerate(posting_lists):\n",
    "            if posting_list.ordered_list[0][1] not in posting_list.docs_visited:\n",
    "                eprint('  Posting list',idx,'is not ready for threshold computation yet, aborting.')\n",
    "                all_lists_ready = False\n",
    "                break\n",
    "            next_prev_scores.append(posting_list.next_item_predecessor_score())\n",
    "        eprint('  Nextprev scores are',next_prev_scores)\n",
    "        threshold = calc_avg(next_prev_scores)\n",
    "    return top_k.top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test Fagin's 2 on ppt example\n",
    "pl1 = PostingList('hello', [(0.9,2),(0.8,5),(0.7,6),(0.6,4),(0.5,1),(0.4,3)])\n",
    "pl2 = PostingList('world', [(0.85,3),(0.8,5),(0.75,2),(0.74,6),(0.74,1),(0.7,4)])\n",
    "\n",
    "print top_k_thresh([pl1, pl2], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Fagins</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posting_lists = util_posting.creat_posting_list_obj_list(\"cat dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultFagin = fagin.top_k_thresh(posting_lists, 4, epsilon=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(resultFagin)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "state": {
    "2ff6ecca697547e0b22000216c90e1e7": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "3d094a44e4194e338ef8f2eb1a8ba493": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "88c1fcf1fe924be8a64d12b379979c1e": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "cfaa7d3d585a4b41b93b277c9ca3bc0d": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
